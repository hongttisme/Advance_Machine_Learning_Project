{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d48b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created 64 nodes\n",
      "created 1792 edge\n",
      "current state (FEN): r1bqkbnr/pppp1ppp/2n5/1B2p3/4P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3\n",
      "\n",
      "encode result:\n",
      "node matrix shape: (64, 21)\n",
      "edge matrix shape: (1792, 11)\n"
     ]
    }
   ],
   "source": [
    "from graph_encode import base_graph_edges,move_to_index,index_to_move,adjacency_list, node_feature_matrix,edge_feature_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a86af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tan04\\AppData\\Local\\Temp\\ipykernel_18008\\3474359750.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_feature_matrix = torch.tensor(node_feature_matrix)\n",
      "C:\\Users\\tan04\\AppData\\Local\\Temp\\ipykernel_18008\\3474359750.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_feature_matrix = torch.tensor(edge_feature_matrix)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "Wu = torch.randn(21, 3)\n",
    "We = torch.randn(11, 3)\n",
    "Wv = torch.randn(21, 3)\n",
    "W0 = torch.randn(21, 21)\n",
    "Wh = torch.randn(21, 21)\n",
    "Wg = torch.randn(11, 21)\n",
    "a = torch.randn(3)\n",
    "\n",
    "node_feature_matrix = torch.tensor(node_feature_matrix)\n",
    "edge_feature_matrix = torch.tensor(edge_feature_matrix)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "\n",
    "hew_i = None\n",
    "for i, neighborhood in enumerate(adjacency_list):\n",
    "    gilist = []\n",
    "    value_list = []\n",
    "    for j, (node_index, edge_index) in enumerate(neighborhood):\n",
    "        gijnew = (\n",
    "            node_feature_matrix[i] @ Wu +\n",
    "            edge_feature_matrix[edge_index] @ We +\n",
    "            node_feature_matrix[node_index] @ Wv\n",
    "        )\n",
    "\n",
    "        gilist.append(leaky_relu(a @ gijnew))\n",
    "        value_list.append(node_feature_matrix[node_index] @ Wh + edge_feature_matrix[edge_index] @ Wg)\n",
    "    gilist = torch.tensor(gilist)\n",
    "    value_list = torch.stack(value_list)\n",
    "    hew_i = node_feature_matrix[i] @ W0 + F.softmax(gilist, dim=0) @ value_list\n",
    "\n",
    "    print(hew_i)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34000c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_nodes = []\n",
    "target_nodes = []\n",
    "edge_feature_indices = []\n",
    "\n",
    "for i, neighborhood in enumerate(adjacency_list):\n",
    "    for neighbor_node, edge_index_val in neighborhood:\n",
    "        target_nodes.append(i)\n",
    "        source_nodes.append(neighbor_node)\n",
    "        edge_feature_indices.append(edge_index_val)\n",
    "\n",
    "edge_index = torch.tensor([target_nodes, source_nodes], dtype=torch.long)\n",
    "edge_map = torch.tensor(edge_feature_indices, dtype=torch.long)\n",
    "num_nodes = len(node_feature_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60414af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GATEAULayer(nn.Module):\n",
    "\n",
    "    def __init__(self, node_in_features, edge_in_features, node_out_features, attention_dim):\n",
    "\n",
    "        super(GATEAULayer, self).__init__()\n",
    "        self.node_in_features = node_in_features\n",
    "        self.edge_in_features = edge_in_features\n",
    "        self.node_out_features = node_out_features\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "  \n",
    "        self.Wv = nn.Parameter(torch.randn(node_in_features, attention_dim))\n",
    "        self.Wu = nn.Parameter(torch.randn(node_in_features, attention_dim))\n",
    "        self.We = nn.Parameter(torch.randn(edge_in_features, attention_dim))\n",
    "        \n",
    "        self.Wh = nn.Parameter(torch.randn(node_in_features, node_out_features))\n",
    "        self.Wg = nn.Parameter(torch.randn(edge_in_features, node_out_features))\n",
    "        self.W0 = nn.Parameter(torch.randn(node_in_features, node_out_features))\n",
    "\n",
    "        self.a = nn.Parameter(torch.randn(attention_dim))\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wu)\n",
    "        nn.init.xavier_uniform_(self.We)\n",
    "        nn.init.xavier_uniform_(self.Wh)\n",
    "        nn.init.xavier_uniform_(self.Wg)\n",
    "        nn.init.xavier_uniform_(self.W0)\n",
    "        nn.init.zeros_(self.a)\n",
    "\n",
    "\n",
    "    def forward(self, node_feature_matrix, edge_feature_matrix, edge_index, edge_map):\n",
    "\n",
    "        num_nodes = node_feature_matrix.shape[0]\n",
    "        target_node_idx, source_node_idx = edge_index[0], edge_index[1]\n",
    "\n",
    "        h_nodes_v = node_feature_matrix @ self.Wv\n",
    "        h_nodes_u = node_feature_matrix @ self.Wu\n",
    "        h_nodes_0 = node_feature_matrix @ self.W0\n",
    "        h_nodes_h = node_feature_matrix @ self.Wh\n",
    "        h_edges_e = edge_feature_matrix @ self.We\n",
    "        h_edges_g = edge_feature_matrix @ self.Wg\n",
    "        \n",
    "\n",
    "        target_node_feats_for_attention = h_nodes_u[target_node_idx]\n",
    "        source_node_feats_for_attention = h_nodes_v[source_node_idx]\n",
    "        edge_feats_for_attention = h_edges_e[edge_map]\n",
    "\n",
    "        attention_scores_raw = target_node_feats_for_attention + source_node_feats_for_attention + edge_feats_for_attention\n",
    "        attention_scores = self.leaky_relu(attention_scores_raw @ self.a) \n",
    "        max_scores = torch.full((num_nodes,), -1e9, device=attention_scores.device, dtype=attention_scores.dtype)\n",
    "        max_scores.scatter_reduce_(0, target_node_idx, attention_scores, reduce=\"amax\", include_self=False)\n",
    "        \n",
    "        scores_max_per_edge = max_scores[target_node_idx]\n",
    "        attention_scores_exp = torch.exp(attention_scores - scores_max_per_edge)\n",
    "\n",
    "        sum_exp_scores = torch.zeros(num_nodes, device=attention_scores.device, dtype=attention_scores.dtype)\n",
    "        sum_exp_scores.index_add_(0, target_node_idx, attention_scores_exp)\n",
    "        \n",
    "        sum_exp_per_edge = sum_exp_scores[target_node_idx]\n",
    "\n",
    "        alpha = attention_scores_exp / (sum_exp_per_edge + 1e-10)\n",
    "\n",
    "\n",
    "        source_node_values = h_nodes_h[source_node_idx]\n",
    "        edge_values = h_edges_g[edge_map]\n",
    "        values = source_node_values + edge_values\n",
    "        \n",
    "        weighted_values = values * alpha.unsqueeze(-1)\n",
    "\n",
    "        aggregated_messages = torch.zeros_like(h_nodes_0)\n",
    "        aggregated_messages.index_add_(0, target_node_idx, weighted_values)\n",
    "\n",
    "\n",
    "        new_final = h_nodes_0 + aggregated_messages\n",
    "\n",
    "        return new_final, attention_scores_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNR(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(BNR, self).__init__()\n",
    "        self.norm = nn.BatchNorm1d(num_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResGATEAU(nn.Module):\n",
    "\n",
    "    def __init__(self, node_in_features, edge_in_features, node_out_features, attention_dim):\n",
    "        super(ResGATEAU, self).__init__()\n",
    "\n",
    "        self.bnr1 = BNR(node_in_features)\n",
    "        self.gateau1 = GATEAULayer(node_in_features, edge_in_features, node_out_features, attention_dim)\n",
    "        \n",
    "\n",
    "        self.bnr2 = BNR(node_out_features)\n",
    "        self.gateau2 = GATEAULayer(node_out_features, edge_in_features, node_out_features, attention_dim)\n",
    "        \n",
    "        if node_in_features != node_out_features:\n",
    "            self.residual_transform = nn.Linear(node_in_features, node_out_features)\n",
    "        else:\n",
    "            self.residual_transform = nn.Identity()\n",
    "\n",
    "    def forward(self, node_feature_matrix, edge_feature_matrix, edge_index, edge_map):\n",
    "        residual = self.residual_transform(node_feature_matrix)\n",
    "\n",
    "\n",
    "        x = self.bnr1(node_feature_matrix)\n",
    "        \n",
    "\n",
    "        x, e = self.gateau1(x, edge_feature_matrix, edge_index, edge_map)\n",
    "        \n",
    "        x = self.bnr2(x)\n",
    "\n",
    "        x, e = self.gateau2(x, e, edge_index, edge_map)\n",
    "\n",
    "\n",
    "        output_node_features = residual + x\n",
    "        \n",
    "        return output_node_features, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df893eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original output shape: torch.Size([21])\n",
      "nn.Module output shape: torch.Size([21])\n",
      "is output same? -> True\n"
     ]
    }
   ],
   "source": [
    "gat_layer = GATEAULayer(\n",
    "    node_in_features=21,\n",
    "    edge_in_features=11,\n",
    "    node_out_features=21,\n",
    "    attention_dim=3\n",
    ")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    gat_layer.Wu.copy_(Wu)\n",
    "    gat_layer.We.copy_(We)\n",
    "    gat_layer.Wv.copy_(Wv)\n",
    "    gat_layer.W0.copy_(W0)\n",
    "    gat_layer.Wh.copy_(Wh)\n",
    "    gat_layer.Wg.copy_(Wg)\n",
    "    gat_layer.a.copy_(a)\n",
    "\n",
    "hew_module_output,_ = gat_layer(\n",
    "    node_feature_matrix=node_feature_matrix,\n",
    "    edge_feature_matrix=edge_feature_matrix,\n",
    "    edge_index=edge_index,\n",
    "    edge_map=edge_map\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "are_close = torch.allclose(hew_i, hew_module_output[0])\n",
    "\n",
    "print(f\"original output shape: {hew_i.shape}\")\n",
    "print(f\"nn.Module output shape: {hew_module_output[0].shape}\")\n",
    "print(f\"is output same? -> {are_close}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
