{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb6ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created 64 nodes\n",
      "created 1792 edge\n",
      "current state (FEN): r1bqkbnr/pppp1ppp/2n5/1B2p3/4P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3\n",
      "\n",
      "encode result:\n",
      "node matrix shape: (64, 21)\n",
      "edge matrix shape: (1792, 11)\n",
      "--- Static Graph Components ---\n",
      "static_edge_index shape: torch.Size([2, 1792])\n",
      "static_edge_map shape: torch.Size([1792])\n",
      "------------------------------\n",
      "成功加载完整数据集，共 684643 条记录。\n",
      "数据划分完成:\n",
      " - 训练集: 547713 条\n",
      " - 验证集: 68465 条\n",
      " - 测试集: 68465 条\n",
      "开始在 cuda 上训练...\n",
      "Checkpoints will be saved in 'checkpoints_v2'\n",
      "Total parameters: 7,354,631\n",
      "Trainable parameters: 7,354,631\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [训练]:   0%|          | 0/4280 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import chess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "# You need to have this file available in your project directory\n",
    "from graph_encode import move_to_index, index_to_move, encode_node_features\n",
    "import matplotlib.pyplot as plt ### NEW ###\n",
    "\n",
    "# Mockup for graph_encode as it was not provided\n",
    "# In your actual code, you would use your own 'graph_encode.py'\n",
    "\n",
    "\n",
    "\n",
    "CSV_FILE_PATH = 'C:\\\\Users\\\\tan04\\\\Documents\\\\codeplace\\\\AI\\\\advance ML\\\\Advance_Machine_Learning_Project\\\\kingbase_processed_all.csv'\n",
    "# CSV_FILE_PATH = 'D:\\\\programming\\\\github\\\\Advance_Machine_Learning_Project\\\\kingbase_processed_all.csv'\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 20\n",
    "MODEL_SAVE_PATH = 'simple_chess_cnn_v2.pth'\n",
    "TEST_SIZE = 0.1\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "### NEW ###\n",
    "# Directory to save model checkpoints\n",
    "CHECKPOINT_DIR = 'checkpoints_v2'\n",
    "\n",
    "NUM_POSSIBLE_MOVES = len(index_to_move)\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "def uci_to_index(uci_move):\n",
    "    try:\n",
    "        # The original code's logic of removing promotion piece might be needed\n",
    "        # depending on the implementation of move_to_index.\n",
    "        move = chess.Move.from_uci(uci_move)\n",
    "        # Assuming move_to_index expects a move without promotion info for lookup\n",
    "        move_without_promotion = chess.Move(move.from_square, move.to_square)\n",
    "        return move_to_index[move_without_promotion]\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding UCI move '{uci_move}': {e}\")\n",
    "        return -1 # Return a sentinel value for errors\n",
    "\n",
    "def state_to_tensor(board: chess.Board):\n",
    "    tensor = encode_node_features(board)\n",
    "    # The original code reshapes a flat (1344,) tensor to (21, 8, 8).\n",
    "    # The mock encode_node_features produces a flat tensor, so we reshape.\n",
    "    return tensor.reshape((21, 8, 8))\n",
    "\n",
    "\n",
    "def result_to_value(result: str):\n",
    "    if result == '1-0': return 1.0\n",
    "    elif result == '0-1': return -1.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        moves_uci = row['Moves_UCI'].split()\n",
    "        result = row['Result']\n",
    "        if len(moves_uci) < 2:\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "        \n",
    "        move_idx_to_play = random.randint(0, len(moves_uci) - 1)\n",
    "        board = chess.Board()\n",
    "        \n",
    "        for move_uci in moves_uci[:move_idx_to_play]:\n",
    "            try:\n",
    "                board.push_uci(move_uci)\n",
    "            except:\n",
    "                return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "        state_tensor = state_to_tensor(board)\n",
    "        target_move_uci = moves_uci[move_idx_to_play]\n",
    "        target_move_index = uci_to_index(target_move_uci)\n",
    "\n",
    "        # Handle cases where uci_to_index failed\n",
    "        if target_move_index == -1:\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "        game_value = result_to_value(result)\n",
    "        if board.turn == chess.BLACK:\n",
    "            game_value = -game_value\n",
    "            \n",
    "        return state_tensor, target_move_index, game_value\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    一个 AlphaZero 风格的残差块\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # 残差连接 (The \"skip connection\")\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class AlphaZeroLikeCNN(nn.Module):\n",
    "    def __init__(self, num_input_channels=21, num_residual_blocks=24, num_filters=128):\n",
    "        super(AlphaZeroLikeCNN, self).__init__()\n",
    "        num_possible_moves = NUM_POSSIBLE_MOVES\n",
    "\n",
    "        # 1. 初始卷积层 (Initial Convolutional Layer)\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, num_filters, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 2. 残差塔 (Tower of Residual Blocks)\n",
    "        self.residual_tower = nn.Sequential(\n",
    "            *[ResidualBlock(num_filters) for _ in range(num_residual_blocks)]\n",
    "        )\n",
    "\n",
    "        # 3. 价值头 (Value Head)\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 1, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8*8, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh() # 输出范围在 [-1, 1]\n",
    "        )\n",
    "\n",
    "        # 4. 策略头 (Policy Head)\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 2, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2 * 8 * 8, num_possible_moves)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的 shape: (batch, 21, 8, 8)\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.residual_tower(x)\n",
    "        \n",
    "        value = self.value_head(x)\n",
    "        policy_logits = self.policy_head(x)\n",
    "        \n",
    "        return value, policy_logits\n",
    "\n",
    "\n",
    "### MODIFIED ###\n",
    "def train_model(model, train_loader, val_loader, epochs, checkpoint_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"开始在 {device} 上训练...\")\n",
    "    model.to(device)\n",
    "\n",
    "    ### NEW ###\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved in '{checkpoint_dir}'\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion_value = nn.MSELoss()\n",
    "    criterion_policy = nn.CrossEntropyLoss()\n",
    "    total, trainable = count_parameters(model)\n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    print(f\"Trainable parameters: {trainable:,}\\n\\n\")\n",
    "\n",
    "    ### NEW ###\n",
    "    # Lists to store loss history for plotting\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Training Part ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [训练]\")\n",
    "        for states, target_moves, target_values in train_pbar:\n",
    "            states, target_moves, target_values = states.to(device), target_moves.to(device), target_values.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            pred_values, pred_policies = model(states)\n",
    "            loss_v = criterion_value(pred_values.squeeze(), target_values)\n",
    "            loss_p = criterion_policy(pred_policies, target_moves)\n",
    "            loss = loss_v + loss_p\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            train_pbar.set_postfix({'train_loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        # --- Validation Part ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [验证]\")\n",
    "            for states, target_moves, target_values in val_pbar:\n",
    "                states, target_moves, target_values = states.to(device), target_moves.to(device), target_values.to(device).float()\n",
    "                pred_values, pred_policies = model(states)\n",
    "                loss_v = criterion_value(pred_values.squeeze(), target_values)\n",
    "                loss_p = criterion_policy(pred_policies, target_moves)\n",
    "                val_loss += (loss_v.item() + loss_p.item())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        ### NEW ###\n",
    "        # Record the losses for this epoch\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} 结束 | 平均训练损失: {avg_train_loss:.4f} | 平均验证损失: {avg_val_loss:.4f}\")\n",
    "\n",
    "        ### NEW ###\n",
    "        # Save a checkpoint after each epoch\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint for epoch {epoch+1} saved to {checkpoint_path}\")\n",
    "\n",
    "\n",
    "    print(\"训练完成！\")\n",
    "    # Save the final model separately for convenience\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"Final model saved to: {MODEL_SAVE_PATH}\")\n",
    "    \n",
    "    ### MODIFIED ###\n",
    "    # Return the loss history\n",
    "    return {'train_loss': train_loss_history, 'val_loss': val_loss_history}\n",
    "\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\n--- 开始在测试集上最终评估 ---\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    criterion_value = nn.MSELoss()\n",
    "    criterion_policy = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(test_loader, desc=\"[测试]\")\n",
    "        for states, target_moves, target_values in test_pbar:\n",
    "            states, target_moves, target_values = states.to(device), target_moves.to(device), target_values.to(device).float()\n",
    "            pred_values, pred_policies = model(states)\n",
    "            loss_v = criterion_value(pred_values.squeeze(), target_values)\n",
    "            loss_p = criterion_policy(pred_policies, target_moves)\n",
    "            test_loss += (loss_v.item() + loss_p.item())\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f\"最终测试损失: {avg_test_loss:.4f}\")\n",
    "    return avg_test_loss\n",
    "\n",
    "\n",
    "### NEW ###\n",
    "def plot_loss_curves(history):\n",
    "    \"\"\"Plots training and validation loss curves.\"\"\"\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_range, history['train_loss'], 'b-o', label='Training Loss')\n",
    "    plt.plot(epochs_range, history['val_loss'], 'r-o', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(CSV_FILE_PATH):\n",
    "        print(f\"错误: 数据文件 {CSV_FILE_PATH} 不存在。请确保路径正确。\")\n",
    "    else:\n",
    "        full_df = pd.read_csv(CSV_FILE_PATH)\n",
    "        print(f\"成功加载完整数据集，共 {len(full_df)} 条记录。\")\n",
    "\n",
    "        # Reduce dataset size for quick testing if needed\n",
    "        # full_df = full_df.sample(n=10000, random_state=42)\n",
    "        # print(f\"Using a smaller sample of {len(full_df)} records for demonstration.\")\n",
    "        \n",
    "        train_val_df, test_df = train_test_split(full_df, test_size=TEST_SIZE, random_state=42)\n",
    "        # Adjust validation split calculation\n",
    "        val_split_ratio = VAL_SIZE / (1 - TEST_SIZE)\n",
    "        train_df, val_df = train_test_split(train_val_df, test_size=val_split_ratio, random_state=42)\n",
    "\n",
    "        print(f\"数据划分完成:\")\n",
    "        print(f\" - 训练集: {len(train_df)} 条\")\n",
    "        print(f\" - 验证集: {len(val_df)} 条\")\n",
    "        print(f\" - 测试集: {len(test_df)} 条\")\n",
    "\n",
    "        train_dataset = ChessDataset(dataframe=train_df)\n",
    "        val_dataset = ChessDataset(dataframe=val_df)\n",
    "        test_dataset = ChessDataset(dataframe=test_df)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) # num_workers can speed up data loading\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "        cnn_model = AlphaZeroLikeCNN()\n",
    "        \n",
    "        ### MODIFIED ###\n",
    "        # Pass the checkpoint directory to the training function\n",
    "        # and capture the returned history\n",
    "        training_history = train_model(cnn_model, train_loader, val_loader, epochs=EPOCHS, checkpoint_dir=CHECKPOINT_DIR)\n",
    "\n",
    "        ### NEW ###\n",
    "        # Plot the captured training and validation losses\n",
    "        if training_history:\n",
    "             plot_loss_curves(training_history)\n",
    "        \n",
    "        # Finally, evaluate the fully trained model on the test set\n",
    "        test_model(cnn_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb81d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
