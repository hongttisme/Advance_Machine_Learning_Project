{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb6ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset split:\n",
      " - testing dataset: 547713 \n",
      " - validation dataset: 68465 \n",
      "train oncuda\n",
      "Checkpoints will be saved in 'checkpoints_cnn'\n",
      "Total parameters: 7,354,631\n",
      "Trainable parameters: 7,354,631\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 [train]:   0%|          | 13/4280 [00:04<24:57,  2.85it/s, train_loss=8.1985]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 287\u001b[39m\n\u001b[32m    282\u001b[39m test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    284\u001b[39m cnn_model = AlphaZeroLikeCNN()\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m training_history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHECKPOINT_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_history:\n\u001b[32m    291\u001b[39m      plot_loss_curves(training_history)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 195\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, checkpoint_dir)\u001b[39m\n\u001b[32m    193\u001b[39m     loss.backward()\n\u001b[32m    194\u001b[39m     optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m     train_pbar.set_postfix({\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m})\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# --- Validation Part ---\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import chess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from graph_encode import move_to_index, index_to_move, encode_node_features\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CSV_FILE_PATH = 'kingbase_processed_all.csv'\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 40\n",
    "MODEL_SAVE_PATH = 'simple_chess_cnn_v2.pth'\n",
    "TEST_SIZE = 0.1\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "\n",
    "CHECKPOINT_DIR = 'checkpoints_cnn'\n",
    "\n",
    "NUM_POSSIBLE_MOVES = len(index_to_move)\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "def uci_to_index(uci_move):\n",
    "    try:\n",
    "\n",
    "        move = chess.Move.from_uci(uci_move)\n",
    "        move_without_promotion = chess.Move(move.from_square, move.to_square)\n",
    "        return move_to_index[move_without_promotion]\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding UCI move '{uci_move}': {e}\")\n",
    "        return -1 \n",
    "\n",
    "def state_to_tensor(board: chess.Board):\n",
    "    tensor = encode_node_features(board)\n",
    "    # reshapes a flat (1344,) tensor to (21, 8, 8).\n",
    "    return tensor.reshape((21, 8, 8))\n",
    "\n",
    "\n",
    "def result_to_value(result: str):\n",
    "    if result == '1-0': return 1.0\n",
    "    elif result == '0-1': return -1.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        moves_uci = row['Moves_UCI'].split()\n",
    "        result = row['Result']\n",
    "        if len(moves_uci) < 2:\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "        \n",
    "        move_idx_to_play = random.randint(0, len(moves_uci) - 1)\n",
    "        board = chess.Board()\n",
    "        \n",
    "        for move_uci in moves_uci[:move_idx_to_play]:\n",
    "            try:\n",
    "                board.push_uci(move_uci)\n",
    "            except:\n",
    "                return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "        state_tensor = state_to_tensor(board)\n",
    "        target_move_uci = moves_uci[move_idx_to_play]\n",
    "        target_move_index = uci_to_index(target_move_uci)\n",
    "\n",
    "        if target_move_index == -1:\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "        game_value = result_to_value(result)\n",
    "        if board.turn == chess.BLACK:\n",
    "            game_value = -game_value\n",
    "            \n",
    "        return state_tensor, target_move_index, game_value\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class AlphaZeroLikeCNN(nn.Module):\n",
    "    def __init__(self, num_input_channels=21, num_residual_blocks=24, num_filters=128):\n",
    "        super(AlphaZeroLikeCNN, self).__init__()\n",
    "        num_possible_moves = NUM_POSSIBLE_MOVES\n",
    "\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, num_filters, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.residual_tower = nn.Sequential(\n",
    "            *[ResidualBlock(num_filters) for _ in range(num_residual_blocks)]\n",
    "        )\n",
    "\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 1, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8*8, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh() \n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 2, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2 * 8 * 8, num_possible_moves)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, 21, 8, 8)\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.residual_tower(x)\n",
    "        \n",
    "        value = self.value_head(x)\n",
    "        policy_logits = self.policy_head(x)\n",
    "        \n",
    "        return value, policy_logits\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, checkpoint_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"train on{device}\")\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved in '{checkpoint_dir}'\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion_value = nn.MSELoss()\n",
    "    criterion_policy = nn.CrossEntropyLoss()\n",
    "    total, trainable = count_parameters(model)\n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    print(f\"Trainable parameters: {trainable:,}\\n\\n\")\n",
    "\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [train]\")\n",
    "        for states, target_moves, target_values in train_pbar:\n",
    "            states, target_moves, target_values = states.to(device), target_moves.to(device), target_values.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            pred_values, pred_policies = model(states)\n",
    "            loss_v = criterion_value(pred_values.squeeze(), target_values)\n",
    "            loss_p = criterion_policy(pred_policies, target_moves)\n",
    "            loss = loss_v + loss_p\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            train_pbar.set_postfix({'train_loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [val]\")\n",
    "            for states, target_moves, target_values in val_pbar:\n",
    "                states, target_moves, target_values = states.to(device), target_moves.to(device), target_values.to(device).float()\n",
    "                pred_values, pred_policies = model(states)\n",
    "                loss_v = criterion_value(pred_values.squeeze(), target_values)\n",
    "                loss_p = criterion_policy(pred_policies, target_moves)\n",
    "                val_loss += (loss_v.item() + loss_p.item())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} end | avg train loss: {avg_train_loss:.4f} | avg val loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint for epoch {epoch+1} saved to {checkpoint_path}\")\n",
    "\n",
    "\n",
    "    print(\"done!\")\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"Final model saved to: {MODEL_SAVE_PATH}\")\n",
    "    \n",
    "\n",
    "    return {'train_loss': train_loss_history, 'val_loss': val_loss_history}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_loss_curves(history):\n",
    "    \"\"\"Plots training and validation loss curves.\"\"\"\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_range, history['train_loss'], 'b-o', label='Training Loss')\n",
    "    plt.plot(epochs_range, history['val_loss'], 'r-o', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(CSV_FILE_PATH):\n",
    "        print(f\"error {CSV_FILE_PATH} not exist\")\n",
    "    else:\n",
    "        full_df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "        \n",
    "        train_val_df, test_df = train_test_split(full_df, test_size=TEST_SIZE, random_state=42)\n",
    "        val_split_ratio = VAL_SIZE / (1 - TEST_SIZE)\n",
    "        train_df, val_df = train_test_split(train_val_df, test_size=val_split_ratio, random_state=42)\n",
    "\n",
    "        print(f\"dataset split:\")\n",
    "        print(f\" - testing dataset: {len(train_df)} \")\n",
    "        print(f\" - validation dataset: {len(val_df)} \")\n",
    "\n",
    "        train_dataset = ChessDataset(dataframe=train_df)\n",
    "        val_dataset = ChessDataset(dataframe=val_df)\n",
    "        test_dataset = ChessDataset(dataframe=test_df)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) # num_workers can speed up data loading\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        cnn_model = AlphaZeroLikeCNN()\n",
    "        \n",
    "\n",
    "        training_history = train_model(cnn_model, train_loader, val_loader, epochs=EPOCHS, checkpoint_dir=CHECKPOINT_DIR)\n",
    "\n",
    "\n",
    "        if training_history:\n",
    "             plot_loss_curves(training_history)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb81d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
